{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8acfef2f",
   "metadata": {},
   "source": [
    "# Градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e07d746d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ЛАБОРАТОРНАЯ РАБОТА №5: ГРАДИЕНТНЫЙ БУСТИНГ\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Импорт библиотек для градиентного бустинга\n",
    "try:\n",
    "    from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "    from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "    HAS_GBM = True\n",
    "except ImportError:\n",
    "    print(\"Установите xgboost и lightgbm: pip install xgboost lightgbm\")\n",
    "    HAS_GBM = False\n",
    "    from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, f1_score, confusion_matrix, classification_report,\n",
    "                             r2_score, mean_absolute_error, mean_squared_error)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ЛАБОРАТОРНАЯ РАБОТА №5: ГРАДИЕНТНЫЙ БУСТИНГ\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed44f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = pd.read_csv('./data/beans/Dry_Bean_Dataset.csv')\n",
    "df_reg = pd.read_csv('./data/football/players_3120.csv')\n",
    "\n",
    "\n",
    "X_cls = df_class.drop('Class', axis=1)\n",
    "y_cls = df_class['Class']\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_cls_encoded = le.fit_transform(y_cls)\n",
    "\n",
    "X_cls_train, X_cls_test, y_cls_train, y_cls_test = train_test_split(\n",
    "    X_cls, y_cls_encoded, test_size=0.2, random_state=42, stratify=y_cls_encoded\n",
    ")\n",
    "\n",
    "scaler_cls = StandardScaler()\n",
    "X_cls_train_scaled = scaler_cls.fit_transform(X_cls_train)\n",
    "X_cls_test_scaled = scaler_cls.transform(X_cls_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "082234e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Используем признаки (22): ['Age', 'Overall rating', 'Potential', 'Height_cm', 'Weight_kg', 'Crossing', 'Finishing', 'Short passing', 'Dribbling', 'Acceleration', 'Sprint speed', 'Stamina', 'Strength', 'Long shots', 'Interceptions', 'Heading accuracy', 'Ball control', 'Reactions', 'Composure', 'Vision', 'Aggression', 'Penalties']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def value_to_float(value_str):\n",
    "    try:\n",
    "        if pd.isna(value_str):\n",
    "            return np.nan\n",
    "        \n",
    "        value_str = str(value_str).strip()\n",
    "        \n",
    "        if value_str.startswith('€'):\n",
    "            value_str = value_str[1:]\n",
    "        \n",
    "        multiplier = 1\n",
    "        if value_str.endswith('M'):\n",
    "            multiplier = 1_000_000\n",
    "            value_str = value_str[:-1]\n",
    "        elif value_str.endswith('K'):\n",
    "            multiplier = 1_000\n",
    "            value_str = value_str[:-1]\n",
    "        \n",
    "        return float(value_str) * multiplier\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка преобразования '{value_str}': {e}\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "df_reg['Value_numeric'] = df_reg['Value'].apply(value_to_float)\n",
    "\n",
    "def height_to_cm(height_str):\n",
    "    try:\n",
    "        if pd.isna(height_str):\n",
    "            return np.nan\n",
    "        height_str = str(height_str)\n",
    "        import re\n",
    "        match = re.search(r'(\\d+)cm', height_str)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "        return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def weight_to_kg(weight_str):\n",
    "    try:\n",
    "        if pd.isna(weight_str):\n",
    "            return np.nan\n",
    "        weight_str = str(weight_str)\n",
    "        import re\n",
    "        match = re.search(r'(\\d+)kg', weight_str)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "        return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df_reg['Height_cm'] = df_reg['Height'].apply(height_to_cm)\n",
    "df_reg['Weight_kg'] = df_reg['Weight'].apply(weight_to_kg)\n",
    "\n",
    "numeric_features = [\n",
    "    'Age', \n",
    "    'Overall rating', \n",
    "    'Potential',\n",
    "    'Height_cm',\n",
    "    'Weight_kg',\n",
    "    'Crossing', \n",
    "    'Finishing', \n",
    "    'Short passing', \n",
    "    'Dribbling',\n",
    "    'Acceleration', \n",
    "    'Sprint speed', \n",
    "    'Stamina', \n",
    "    'Strength',\n",
    "    'Long shots',\n",
    "    'Interceptions',\n",
    "    'Heading accuracy',\n",
    "    'Ball control',\n",
    "    'Reactions',\n",
    "    'Composure',\n",
    "    'Vision',\n",
    "    'Aggression',\n",
    "    'Penalties'\n",
    "]\n",
    "\n",
    "existing_features = [col for col in numeric_features if col in df_reg.columns]\n",
    "print(f\"\\nИспользуем признаки ({len(existing_features)}): {existing_features}\")\n",
    "\n",
    "X_reg = df_reg[existing_features].copy()\n",
    "\n",
    "for col in X_reg.columns:\n",
    "    X_reg[col] = pd.to_numeric(X_reg[col], errors='coerce')\n",
    "\n",
    "X_reg = X_reg.fillna(X_reg.median())\n",
    "\n",
    "y_reg = df_reg['Value_numeric']\n",
    "\n",
    "valid_indices = y_reg.notna()\n",
    "X_reg = X_reg[valid_indices]\n",
    "y_reg = y_reg[valid_indices]\n",
    "\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_reg = StandardScaler()\n",
    "X_reg_train_scaled = scaler_reg.fit_transform(X_reg_train)\n",
    "X_reg_test_scaled = scaler_reg.transform(X_reg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a37e8",
   "metadata": {},
   "source": [
    "# 2.1 Классификация "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e318bd",
   "metadata": {},
   "source": [
    "Обучение бейзлайна и оценка качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c83a8c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "КЛАССИФИКАЦИЯ: Градиентный бустинг (бейзлайн)\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      5\u001b[0m gb_cls_baseline \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier(\n\u001b[1;32m      6\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      7\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      8\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      9\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mgb_cls_baseline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_cls_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_cls_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m y_cls_pred_gb \u001b[38;5;241m=\u001b[39m gb_cls_baseline\u001b[38;5;241m.\u001b[39mpredict(X_cls_test_scaled)\n\u001b[1;32m     14\u001b[0m y_cls_prob_gb \u001b[38;5;241m=\u001b[39m gb_cls_baseline\u001b[38;5;241m.\u001b[39mpredict_proba(X_cls_test_scaled)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_gb.py:787\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_gb.py:883\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    876\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[1;32m    877\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[1;32m    878\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    879\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[1;32m    880\u001b[0m         )\n\u001b[1;32m    882\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 883\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_gb.py:489\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    486\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    488\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 489\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[1;32m    494\u001b[0m X_for_tree_update \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/tree/_classes.py:1404\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \n\u001b[1;32m   1378\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1404\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"КЛАССИФИКАЦИЯ: Градиентный бустинг (бейзлайн)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gb_cls_baseline = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_cls_baseline.fit(X_cls_train_scaled, y_cls_train)\n",
    "y_cls_pred_gb = gb_cls_baseline.predict(X_cls_test_scaled)\n",
    "y_cls_prob_gb = gb_cls_baseline.predict_proba(X_cls_test_scaled)\n",
    "\n",
    "acc_gb_cls = accuracy_score(y_cls_test, y_cls_pred_gb)\n",
    "f1_gb_cls = f1_score(y_cls_test, y_cls_pred_gb, average='macro')\n",
    "\n",
    "print(f\"Accuracy (GradientBoosting): {acc_gb_cls:.4f}\")\n",
    "print(f\"F1-score (macro): {f1_gb_cls:.4f}\")\n",
    "\n",
    "feature_importance_gb_cls = pd.DataFrame({\n",
    "    'feature': X_cls.columns,\n",
    "    'importance': gb_cls_baseline.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nТоп-10 самых важных признаков (градиентный бустинг, классификация):\")\n",
    "print(feature_importance_gb_cls.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9cda5",
   "metadata": {},
   "source": [
    "# 2.1 Регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c4f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "РЕГРЕССИЯ: Градиентный бустинг (бейзлайн)\n",
      "======================================================================\n",
      "R² (GradientBoosting): 0.7741\n",
      "MAE: 3,419,870 евро (3.42 млн евро)\n",
      "RMSE: 7,922,704 евро (7.92 млн евро)\n",
      "\n",
      "Топ-10 самых важных признаков (градиентный бустинг, регрессия):\n",
      "           feature  importance\n",
      "2        Potential    0.175547\n",
      "17       Reactions    0.141347\n",
      "0              Age    0.133887\n",
      "18       Composure    0.084538\n",
      "11         Stamina    0.049669\n",
      "7    Short passing    0.047647\n",
      "1   Overall rating    0.044960\n",
      "19          Vision    0.035541\n",
      "20      Aggression    0.035392\n",
      "14   Interceptions    0.033543\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"РЕГРЕССИЯ: Градиентный бустинг (бейзлайн)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gb_reg_baseline = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_reg_train_log = np.log1p(y_reg_train)\n",
    "\n",
    "gb_reg_baseline.fit(X_reg_train_scaled, y_reg_train_log)\n",
    "\n",
    "y_reg_pred_log_gb = gb_reg_baseline.predict(X_reg_test_scaled)\n",
    "y_reg_pred_gb = np.exp(y_reg_pred_log_gb) - 1\n",
    "\n",
    "r2_gb_reg = r2_score(y_reg_test, y_reg_pred_gb)\n",
    "mae_gb_reg = mean_absolute_error(y_reg_test, y_reg_pred_gb)\n",
    "rmse_gb_reg = np.sqrt(mean_squared_error(y_reg_test, y_reg_pred_gb))\n",
    "\n",
    "print(f\"R² (GradientBoosting): {r2_gb_reg:.4f}\")\n",
    "print(f\"MAE: {mae_gb_reg:,.0f} евро ({mae_gb_reg/1_000_000:.2f} млн евро)\")\n",
    "print(f\"RMSE: {rmse_gb_reg:,.0f} евро ({rmse_gb_reg/1_000_000:.2f} млн евро)\")\n",
    "\n",
    "feature_importance_gb_reg = pd.DataFrame({\n",
    "    'feature': X_reg.columns,\n",
    "    'importance': gb_reg_baseline.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nТоп-10 самых важных признаков (градиентный бустинг, регрессия):\")\n",
    "print(feature_importance_gb_reg.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d13507",
   "metadata": {},
   "source": [
    "# 3 Улучшение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f1582",
   "metadata": {},
   "source": [
    "### Для классификации:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df2987",
   "metadata": {},
   "source": [
    "\n",
    "H1: learning_rate=0.05 + n_estimators=200 → лучшая сходимость и точность\n",
    "\n",
    "H2: subsample=0.8 → стохастический GB, меньше переобучение\n",
    "\n",
    "H3: max_depth=5 + min_samples_split=10 → баланс смещения и дисперсии\n",
    "\n",
    "H4: min_samples_leaf=5 → дополнительная регуляризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2ba09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "КЛАССИФИКАЦИЯ: Улучшение градиентного бустинга\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Результаты улучшенного GradientBoosting:\n",
      "Accuracy: 0.9232\n",
      "F1-score (macro): 0.9357\n",
      "Улучшение по сравнению с бейзлайном: +0.0004\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ПРОВЕРКА ГИПОТЕЗЫ H4: XGBoost/LightGBM vs sklearn GBM\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koche/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [21:30:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost с аналогичными параметрами:\n",
      "  Accuracy: 0.9258\n",
      "  Разница с sklearn GBM: +0.0026\n",
      "✓ ГИПОТЕЗА H4 ПОДТВЕРЖДЕНА: XGBoost показал лучший результат\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"КЛАССИФИКАЦИЯ: Улучшение градиентного бустинга\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "\n",
    "gb_cls_improved = GradientBoostingClassifier(\n",
    "    n_estimators=200,          # H1: Больше деревьев с меньшим learning_rate\n",
    "    learning_rate=0.05,        # H1: Меньшая скорость обучения для лучшей сходимости\n",
    "    max_depth=5,               # H3: Глубже деревья для сложных паттернов\n",
    "    min_samples_split=10,      # H3: Регуляризация через min_samples_split\n",
    "    min_samples_leaf=5,        # H4: Дополнительная регуляризация через min_samples_leaf\n",
    "    subsample=0.8,             # H2: Stochastic Gradient Boosting\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "gb_cls_improved.fit(X_cls_train_scaled, y_cls_train)\n",
    "y_cls_pred_gb_imp = gb_cls_improved.predict(X_cls_test_scaled)\n",
    "\n",
    "acc_gb_cls_imp = accuracy_score(y_cls_test, y_cls_pred_gb_imp)\n",
    "f1_gb_cls_imp = f1_score(y_cls_test, y_cls_pred_gb_imp, average='macro')\n",
    "\n",
    "print(f\"\\nРезультаты улучшенного GradientBoosting:\")\n",
    "print(f\"Accuracy: {acc_gb_cls_imp:.4f}\")\n",
    "print(f\"F1-score (macro): {f1_gb_cls_imp:.4f}\")\n",
    "print(f\"Улучшение по сравнению с бейзлайном: {acc_gb_cls_imp - acc_gb_cls:+.4f}\")\n",
    "\n",
    "# Проверяем гипотезу H4: XGBoost/LightGBM лучше sklearn GBM\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"ПРОВЕРКА ГИПОТЕЗЫ H4: XGBoost/LightGBM vs sklearn GBM\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "if HAS_GBM:\n",
    "    xgb_cls_comparison = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        min_child_weight=5,  \n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    \n",
    "    xgb_cls_comparison.fit(X_cls_train_scaled, y_cls_train)\n",
    "    acc_xgb = accuracy_score(y_cls_test, xgb_cls_comparison.predict(X_cls_test_scaled))\n",
    "    \n",
    "    print(f\"XGBoost с аналогичными параметрами:\")\n",
    "    print(f\"  Accuracy: {acc_xgb:.4f}\")\n",
    "    print(f\"  Разница с sklearn GBM: {acc_xgb - acc_gb_cls_imp:+.4f}\")\n",
    "    \n",
    "    if acc_xgb > acc_gb_cls_imp:\n",
    "        print(\"✓ ГИПОТЕЗА H4 ПОДТВЕРЖДЕНА: XGBoost показал лучший результат\")\n",
    "    else:\n",
    "        print(\"✗ ГИПОТЕЗА H4 НЕ ПОДТВЕРЖДЕНА: sklearn GBM показал сравнимый результат\")\n",
    "else:\n",
    "    print(\"XGBoost не установлен. Установите: pip install xgboost\")\n",
    "    print(\"Для проверки H4 требуется установка дополнительных библиотек\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb89c2",
   "metadata": {},
   "source": [
    "## Для регрессии:\n",
    "\n",
    "H1: Увеличение n_estimators до 200 улучшит качество (vs 100 в бейзлайне)\n",
    "\n",
    "H2: Уменьшение learning_rate до 0.05 улучшит сходимость (vs 0.1 в бейзлайне)\n",
    "\n",
    "H3: Увеличение max_depth до 5 улучшит качество (vs 3 в бейзлайне)\n",
    "\n",
    "H4: Использование subsample=0.8 улучшит обобщающую способность\n",
    "\n",
    "H5: XGBoost покажет лучшие результаты, чем sklearn GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f13ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "РЕГРЕССИЯ: Улучшение градиентного бустинга\n",
      "----------------------------------------------------------------------\n",
      "Улучшенный GradientBoosting (регрессия):\n",
      "R²: 0.8310\n",
      "MAE: 3,043,914 евро (3.04 млн евро)\n",
      "Улучшение R² по сравнению с бейзлайном: +0.0569\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "XGBOOST (продвинутый градиентный бустинг)\n",
      "----------------------------------------------------------------------\n",
      "XGBoost Regressor R²: 0.8114\n",
      "Улучшение vs sklearn GBM (регрессия): +0.0373\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"РЕГРЕССИЯ: Улучшение градиентного бустинга\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "gb_reg_improved = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_reg_improved.fit(X_reg_train_scaled, y_reg_train_log)\n",
    "\n",
    "y_reg_pred_log_gb_imp = gb_reg_improved.predict(X_reg_test_scaled)\n",
    "y_reg_pred_gb_imp = np.exp(y_reg_pred_log_gb_imp) - 1\n",
    "\n",
    "r2_gb_reg_imp = r2_score(y_reg_test, y_reg_pred_gb_imp)\n",
    "mae_gb_reg_imp = mean_absolute_error(y_reg_test, y_reg_pred_gb_imp)\n",
    "rmse_gb_reg_imp = np.sqrt(mean_squared_error(y_reg_test, y_reg_pred_gb_imp))\n",
    "\n",
    "print(f\"Улучшенный GradientBoosting (регрессия):\")\n",
    "print(f\"R²: {r2_gb_reg_imp:.4f}\")\n",
    "print(f\"MAE: {mae_gb_reg_imp:,.0f} евро ({mae_gb_reg_imp/1_000_000:.2f} млн евро)\")\n",
    "print(f\"Улучшение R² по сравнению с бейзлайном: {r2_gb_reg_imp - r2_gb_reg:+.4f}\")\n",
    "\n",
    "if HAS_GBM:\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"XGBOOST (продвинутый градиентный бустинг)\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    xgb_reg = XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb_reg.fit(X_reg_train_scaled, y_reg_train_log)\n",
    "    y_reg_pred_log_xgb = xgb_reg.predict(X_reg_test_scaled)\n",
    "    y_reg_pred_xgb = np.exp(y_reg_pred_log_xgb) - 1\n",
    "    r2_xgb_reg = r2_score(y_reg_test, y_reg_pred_xgb)\n",
    "    \n",
    "    print(f\"XGBoost Regressor R²: {r2_xgb_reg:.4f}\")\n",
    "    print(f\"Улучшение vs sklearn GBM (регрессия): {r2_xgb_reg - r2_gb_reg:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a557b17",
   "metadata": {},
   "source": [
    "# ИМПЛЕМЕНТАЦИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "class GradientBoosting:\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.05, max_depth=5, \n",
    "                 min_samples_split=10, min_samples_leaf=5, subsample=0.8, \n",
    "                 random_state=None, verbose=0):\n",
    "        \"\"\"\n",
    "        УЛУЧШЕННЫЕ параметры (согласно вашим улучшениям):\n",
    "        - n_estimators: 200 (H1: больше деревьев)\n",
    "        - learning_rate: 0.05 (H1: меньшая скорость обучения)\n",
    "        - max_depth: 5 (H3: глубже деревья)\n",
    "        - min_samples_split: 10 (H3: регуляризация)\n",
    "        - min_samples_leaf: 5 (H4: дополнительная регуляризация)\n",
    "        - subsample: 0.8 (H2: Stochastic Gradient Boosting)\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.subsample = subsample\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.trees = []\n",
    "        self.train_errors = []\n",
    "        \n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "    \n",
    "    def _subsample(self, X, y):\n",
    "        \"\"\"Создание подвыборки для обучения дерева\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        sample_size = int(n_samples * self.subsample)\n",
    "        indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
    "        return X[indices], y[indices]  # Правильное индексирование для numpy массивов\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучение модели\"\"\"\n",
    "        raise NotImplementedError(\"Это абстрактный метод\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказание\"\"\"\n",
    "        raise NotImplementedError(\"Это абстрактный метод\")\n",
    "\n",
    "class GradientBoostingRegressor(GradientBoosting):\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.05, max_depth=5, \n",
    "                 min_samples_split=10, min_samples_leaf=5, subsample=0.8, \n",
    "                 random_state=None, verbose=0):\n",
    "        super().__init__(n_estimators, learning_rate, max_depth, \n",
    "                        min_samples_split, min_samples_leaf, subsample, \n",
    "                        random_state, verbose)\n",
    "        self.initial_prediction = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучение модели регрессии\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        self.initial_prediction = np.mean(y)\n",
    "        predictions = np.full(y.shape, self.initial_prediction)\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            residuals = y - predictions\n",
    "            \n",
    "            tree = DecisionTreeRegressor(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "            \n",
    "            if self.subsample < 1.0:\n",
    "                X_subsample, residuals_subsample = self._subsample(X, residuals)\n",
    "                tree.fit(X_subsample, residuals_subsample)\n",
    "            else:\n",
    "                tree.fit(X, residuals)\n",
    "            \n",
    "            tree_prediction = tree.predict(X)\n",
    "            predictions += self.learning_rate * tree_prediction\n",
    "            \n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            mse = mean_squared_error(y, predictions)\n",
    "            self.train_errors.append(mse)\n",
    "            \n",
    "\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказание для регрессии\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        predictions = np.full(X.shape[0], self.initial_prediction)\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "class GradientBoostingClassifier(GradientBoosting):\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.05, max_depth=5, \n",
    "                 min_samples_split=10, min_samples_leaf=5, subsample=0.8, \n",
    "                 random_state=None, verbose=0):\n",
    "        super().__init__(n_estimators, learning_rate, max_depth, \n",
    "                        min_samples_split, min_samples_leaf, subsample, \n",
    "                        random_state, verbose)\n",
    "        self.initial_prediction = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        \"\"\"Сигмоидная функция\"\"\"\n",
    "        x = np.clip(x, -10, 10)  \n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _log_loss_gradient(y_true, y_pred):\n",
    "        \"\"\"Градиент логистической функции потерь\"\"\"\n",
    "        return y_true - GradientBoostingClassifier._sigmoid(y_pred)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучение модели классификации (бинарной)\"\"\"\n",
    "        # if self.verbose > 0:\n",
    "        #     print(f\"Обучаем GradientBoostingClassifier ({self.n_estimators} деревьев)...\")\n",
    "        #     print(f\"Параметры: n_estimators={self.n_estimators}, lr={self.learning_rate}\")\n",
    "        \n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        y_binary = np.where(y > 0, 1, 0)\n",
    "        \n",
    "        pos_prob = np.mean(y_binary)\n",
    "        self.initial_prediction = np.log(pos_prob / (1 - pos_prob + 1e-10))\n",
    "        log_odds = np.full(y_binary.shape, self.initial_prediction)\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            gradients = self._log_loss_gradient(y_binary, log_odds)\n",
    "            \n",
    "            tree = DecisionTreeRegressor(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "            \n",
    "            if self.subsample < 1.0:\n",
    "                X_subsample, gradients_subsample = self._subsample(X, gradients)\n",
    "                tree.fit(X_subsample, gradients_subsample)\n",
    "            else:\n",
    "                tree.fit(X, gradients)\n",
    "            \n",
    "            tree_prediction = tree.predict(X)\n",
    "            log_odds += self.learning_rate * tree_prediction\n",
    "            \n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            probabilities = self._sigmoid(log_odds)\n",
    "            predictions = (probabilities > 0.5).astype(int)\n",
    "            accuracy = accuracy_score(y_binary, predictions)\n",
    "            self.train_errors.append(accuracy)\n",
    "            \n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Предсказание вероятностей\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        log_odds = np.full(X.shape[0], self.initial_prediction)\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            log_odds += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        probabilities = self._sigmoid(log_odds)\n",
    "        return np.column_stack([1 - probabilities, probabilities])\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Предсказание классов\"\"\"\n",
    "        probabilities = self.predict_proba(X)[:, 1]\n",
    "        return (probabilities > threshold).astype(int)\n",
    "\n",
    "class GradientBoostingMulticlassClassifier:\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.05, max_depth=5, \n",
    "                 min_samples_split=10, min_samples_leaf=5, subsample=0.8, \n",
    "                 random_state=None, verbose=0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.subsample = subsample\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.classifiers = []\n",
    "        self.classes_ = None\n",
    "        \n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучение для многоклассовой классификации (One-vs-All)\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        # if self.verbose > 0:\n",
    "        #     print(f\"Обучаем многоклассовый GradientBoosting ({n_classes} классов)...\")\n",
    "        #     print(f\"Параметры: n_estimators={self.n_estimators}, lr={self.learning_rate}\")\n",
    "        \n",
    "        for i, class_label in enumerate(self.classes_):\n",
    "            # if self.verbose > 0:\n",
    "            #     print(f\"\\nКласс {class_label} ({i+1}/{n_classes}):\")\n",
    "            \n",
    "            y_binary = np.where(y == class_label, 1, 0)\n",
    "            \n",
    "            clf = GradientBoostingClassifier(\n",
    "                n_estimators=self.n_estimators,\n",
    "                learning_rate=self.learning_rate,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                subsample=self.subsample,\n",
    "                random_state=self.random_state,\n",
    "                verbose=self.verbose - 1 if self.verbose > 1 else 0\n",
    "            )\n",
    "            clf.fit(X, y_binary)\n",
    "            \n",
    "            self.classifiers.append(clf)\n",
    "        \n",
    "        # if self.verbose > 0:\n",
    "        #     print(f\"\\nОбучение всех {n_classes} классификаторов завершено!\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Предсказание вероятностей для всех классов\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes_)\n",
    "        probabilities = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        for i, clf in enumerate(self.classifiers):\n",
    "            probabilities[:, i] = clf.predict_proba(X)[:, 1]\n",
    "        \n",
    "\n",
    "        prob_sum = probabilities.sum(axis=1, keepdims=True)\n",
    "        prob_sum[prob_sum == 0] = 1\n",
    "        probabilities = probabilities / prob_sum\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказание классов\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probabilities, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19883fe",
   "metadata": {},
   "source": [
    "## Обучение и оценка имплементированных моделей:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0acfc",
   "metadata": {},
   "source": [
    "Обучение собственной модели и оценка (классификация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52204729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ОБУЧЕНИЕ СОБСТВЕННЫХ РЕАЛИЗАЦИЙ ГРАДИЕНТНОГО БУСТИНГА\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Градиентный бустинг для классификации (наша реализация)\n",
      "----------------------------------------------------------------------\n",
      "Наш GradientBoosting (классификация):\n",
      "Accuracy: 0.8902\n",
      "F1-score: 0.9035\n",
      "Сравнение с sklearn: -0.0327\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ОБУЧЕНИЕ СОБСТВЕННЫХ РЕАЛИЗАЦИЙ ГРАДИЕНТНОГО БУСТИНГА\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Градиентный бустинг для классификации (наша реализация)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "my_gb_cls = GradientBoostingMulticlassClassifier(\n",
    "    n_estimators=30,      # Мало деревьев для скорости\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    min_samples_split=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "my_gb_cls.fit(X_cls_train_scaled, y_cls_train)\n",
    "y_cls_pred_my_gb = my_gb_cls.predict(X_cls_test_scaled)\n",
    "\n",
    "acc_my_gb_cls = accuracy_score(y_cls_test, y_cls_pred_my_gb)\n",
    "f1_my_gb_cls = f1_score(y_cls_test, y_cls_pred_my_gb, average='macro')\n",
    "\n",
    "print(f\"Наш GradientBoosting (классификация):\")\n",
    "print(f\"Accuracy: {acc_my_gb_cls:.4f}\")\n",
    "print(f\"F1-score: {f1_my_gb_cls:.4f}\")\n",
    "print(f\"Сравнение с sklearn: {acc_my_gb_cls - acc_gb_cls:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf901040",
   "metadata": {},
   "source": [
    "Обучение собственной модели и оценка (регрессия)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8e129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "РЕГРЕССИЯ: Ваш GradientBoostingRegressor\n",
      "----------------------------------------------------------------------\n",
      "Обучаем ваш регрессор...\n",
      "\n",
      "Ваш GradientBoostingRegressor:\n",
      "R²: 0.7692\n",
      "MAE: 3.70 млн евро\n",
      "Сравнение с sklearn GBM бейзлайн: -0.0049\n",
      "Сравнение с улучшенным sklearn GBM: -0.0618\n"
     ]
    }
   ],
   "source": [
    "print(\"РЕГРЕССИЯ: Ваш GradientBoostingRegressor\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "gb_reg_your = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Обучаем ваш регрессор...\")\n",
    "gb_reg_your.fit(X_reg_train_scaled, y_reg_train_log)\n",
    "\n",
    "# Предсказание\n",
    "y_reg_pred_log_your = gb_reg_your.predict(X_reg_test_scaled)\n",
    "y_reg_pred_your = np.exp(y_reg_pred_log_your) - 1\n",
    "\n",
    "r2_your_gb_reg = r2_score(y_reg_test, y_reg_pred_your)\n",
    "mae_your_gb_reg = mean_absolute_error(y_reg_test, y_reg_pred_your)\n",
    "\n",
    "print(f\"\\nВаш GradientBoostingRegressor:\")\n",
    "print(f\"R²: {r2_your_gb_reg:.4f}\")\n",
    "print(f\"MAE: {mae_your_gb_reg/1_000_000:.2f} млн евро\")\n",
    "print(f\"Сравнение с sklearn GBM бейзлайн: {r2_your_gb_reg - r2_gb_reg:+.4f}\")\n",
    "print(f\"Сравнение с улучшенным sklearn GBM: {r2_your_gb_reg - r2_gb_reg_imp:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2133bef7",
   "metadata": {},
   "source": [
    "## улучшенные парамметры"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebcaf0",
   "metadata": {},
   "source": [
    "Обучение собственной модели и оценка (классификация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d88ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ТЕСТИРОВАНИЕ НАШЕЙ РЕАЛИЗАЦИИ С ПАРАМЕТРАМИ ИЗ УЛУЧШЕНИЙ\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "КЛАССИФИКАЦИЯ: Наш Gradient Boosting\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Наш GradientBoosting (классификация):\n",
      "Accuracy: 0.9082\n",
      "F1-score: 0.9217\n",
      "Сравнение с sklearn GBM бейзлайн: -0.0147\n",
      "Сравнение с улучшенным sklearn GBM: -0.0151\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ТЕСТИРОВАНИЕ НАШЕЙ РЕАЛИЗАЦИИ С ПАРАМЕТРАМИ ИЗ УЛУЧШЕНИЙ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"КЛАССИФИКАЦИЯ: Наш Gradient Boosting\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "my_gb_cls_test = GradientBoostingMulticlassClassifier(\n",
    "    n_estimators=50,  # Уменьшаем для скорости демонстрации\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "my_gb_cls_test.fit(X_cls_train_scaled, y_cls_train)\n",
    "\n",
    "y_cls_pred_my_gb = my_gb_cls_test.predict(X_cls_test_scaled)\n",
    "acc_my_gb_cls = accuracy_score(y_cls_test, y_cls_pred_my_gb)\n",
    "f1_my_gb_cls = f1_score(y_cls_test, y_cls_pred_my_gb, average='macro')\n",
    "\n",
    "print(f\"\\nНаш GradientBoosting (классификация):\")\n",
    "print(f\"Accuracy: {acc_my_gb_cls:.4f}\")\n",
    "print(f\"F1-score: {f1_my_gb_cls:.4f}\")\n",
    "print(f\"Сравнение с sklearn GBM бейзлайн: {acc_my_gb_cls - acc_gb_cls:+.4f}\")\n",
    "print(f\"Сравнение с улучшенным sklearn GBM: {acc_my_gb_cls - acc_gb_cls_imp:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd071398",
   "metadata": {},
   "source": [
    "Обучение собственной модели и оценка (регрессия)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "РЕГРЕССИЯ: Ваша реализация с улучшенными параметрами\n",
      "----------------------------------------------------------------------\n",
      "Обучаем ваш регрессор...\n",
      "\n",
      "Результаты вашей реализации (регрессия):\n",
      "R²: 0.7692\n",
      "MAE: 3.70 млн евро\n",
      "Сравнение с sklearn GBM бейзлайн: -0.0049\n",
      "Сравнение с улучшенным sklearn GBM: -0.0618\n"
     ]
    }
   ],
   "source": [
    "print(\"РЕГРЕССИЯ: Ваша реализация с улучшенными параметрами\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "gb_reg_your = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Обучаем ваш регрессор...\")\n",
    "gb_reg_your.fit(X_reg_train_scaled, y_reg_train_log)\n",
    "\n",
    "# Предсказание\n",
    "y_reg_pred_log_your = gb_reg_your.predict(X_reg_test_scaled)\n",
    "y_reg_pred_your = np.exp(y_reg_pred_log_your) - 1\n",
    "\n",
    "r2_your_reg = r2_score(y_reg_test, y_reg_pred_your)\n",
    "mae_your_reg = mean_absolute_error(y_reg_test, y_reg_pred_your)\n",
    "\n",
    "print(f\"\\nРезультаты вашей реализации (регрессия):\")\n",
    "print(f\"R²: {r2_your_reg:.4f}\")\n",
    "print(f\"MAE: {mae_your_reg/1_000_000:.2f} млн евро\")\n",
    "print(f\"Сравнение с sklearn GBM бейзлайн: {r2_your_reg - r2_gb_reg:+.4f}\")\n",
    "print(f\"Сравнение с улучшенным sklearn GBM: {r2_your_reg - r2_gb_reg_imp:+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
